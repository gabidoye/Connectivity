import boto3
import awswrangler as wr
raw_bucket = 'salesanalyticssparkjob'
raw_path_dir = 'Sales'
raw_path = f"s3://{raw_bucket}/{raw_path_dir}"


df_csv = wr.s3.read_csv(path=raw_path, path_suffix=['.csv'],dataset=True)

s3 = boto3.resource('s3',
         aws_access_key_id='xxxxxxxxxxxxxxx',
         aws_secret_access_key= 'xxxxxxxxxxxxx')


df_csv.head()



##### Write to ODBC
#Write data to database
#./bin/pyspark --packages org.postgresql:postgresql:42.1.1

jdbcDF1 = spark.read.format("jdbc").option("url", "jdbc:postgresql:dvdrental") .option("dbtable", "public.actor") .option("user", "postgres") .option("password", "Oracle987").load()
        
jdbcDF2 = (spark.read.format("jdbc")\
        .option("url", "jdbc:postgresql://localhost:5432/dvdrental") .option("dbtable", "public.actor")\
        .option("user", "postgres") .option("password", "Oracle987")\
        .load())


